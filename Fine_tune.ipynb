{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyo0914/ML/blob/main/Fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU Availability"
      ],
      "metadata": {
        "id": "0FOLEQCjGRCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "AiN2sgWYGDyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47c36ea-ee16-42e4-d824-6c5da8c8fca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr 17 10:50:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj1bnkewjzjx"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V9cJgcJjzjy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
        "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install unsloth==2025.2.15 unsloth_zoo==2025.2.7\n",
        "!pip install transformers==4.49.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ericsunkuan/ML_Spring2025_HW5.git"
      ],
      "metadata": {
        "id": "E2i1eDs-NM2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc950de-f917-4b44-c621-a0c8402e8a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML_Spring2025_HW5'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 27 (delta 10), reused 21 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (27/27), 12.73 MiB | 8.89 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPfTaySmjzjz"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note : Changing the model is against the Rules of Homework 5 !!!"
      ],
      "metadata": {
        "id": "WVqNG3cME9r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the LLM"
      ],
      "metadata": {
        "id": "EadFCbszFUkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zymmx7qsjzjz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357,
          "referenced_widgets": [
            "f14b458144df4983a4a4f19873b00c02",
            "b0fa9aa15a1f4760a85e62c8ac523871",
            "45adbcf1c7994688bfe7a742d2747ed4",
            "a30147c022df402f8bbe7ee4b142814b",
            "2cfe5b1327f44e28ba7c88922ca4ec9d",
            "17b21ca13b0b4f298f2d6d0cc5cff618",
            "dd96db014d534ebca4e1036daabac5a9",
            "4603a7ca75094cb7acb712e6a9d44445",
            "7229cdcaeff048e691df27e282378c12",
            "8412f54e56d345d2862aeec294091a3a",
            "f6eddab1095242e09c69c264b1e7f5e0",
            "2213bbac271d4cdc81650686ddc61dd0",
            "6ff30149c05b49d3ae3488eb36f9be7f",
            "d9297fc0c6df48d0ab3e3c0df40137a6",
            "395a5e8c67d549f6ba947f1a363be926",
            "8f5c9508aa07448f83bb7ca43f59bc7d",
            "c17576cdacd04fedae1df91f2ab173d1",
            "c7dd2e46810547469c4c10777dc9ca34",
            "4bf035d85ce0406390057ce7788db330",
            "46208ead76fd48edb196e3610adc41f7",
            "b50fb252e74649f5a61792519a63c308",
            "5252b0454f73485d87325de705d5710d",
            "0933da2aba3e4a31b2c62f078471a2a6",
            "bde9076bb0a04a35bd1fa41115b9a7a7",
            "c4583c490147413a923ab52b621d4cd6",
            "bdd653b3816546d890e0a58135ad660c",
            "9ea9c3861c544406bbbbc3c3271ec9ca",
            "79eb382cb55c4c149a7f712212043fe1",
            "5a969a3db60f463fa4a5aee4560d5257",
            "e63cae23e9094256a919078bd9d4482f",
            "fdff8645e4f44d41ae7aeaa0a6721f7d",
            "24897feb4b0b41ca92925ffdd97c0628",
            "0e8605b6f1934f30bb3a8a56f489df1e",
            "c48985f99cb547389f4536e7c4453a90",
            "2a89dcb7cf154c33b69c2513bd5c2ea0",
            "8513be04fa834a9f840cf69912eaf9b6",
            "bc186aa494a14391b88a80cbc4440466",
            "482cf8abbf0649cd9ae9d56671c72cca",
            "e8d64c86f2984be1a992fe67d4671461",
            "7a510e2df9ec461c8b51ca92dba44404",
            "5e1148d67f53494ea09a2cbfefcbf1e2",
            "291bc9063cd84154b7d9a9cbf377fa92",
            "033dca47fc5549609515368001065e5b",
            "25665d31bc534b7a9a4bbfeadbc608d2",
            "47cbbc21616147e8bedec1c6cf170201",
            "3c78a8da77844240bfd293f149cfb661",
            "37a64afded794b17b6f024dfd7bf85dc",
            "8998caaa3b6545dc87b0f8b4c7076b9b",
            "1ad85d9dbf7343dd87d534dad48c4d49",
            "08512f0cee3748a1b13b289d644e7860",
            "4c213252447a4a9c9a0a4b4c17e3b00b",
            "7b920f7450bb4f8883015d9026c920c5",
            "04dfccb7b3aa420bbb8946102633c85f",
            "f95a92d483694b128afad949101879e8",
            "0ded620d82924519a3d4fdccf47b6d45",
            "91822aea5ff74fc396af181260e19211",
            "72812e2d054446b4958c9fafd61fa930",
            "0f849382332e4fed9f86553f9b4a71a3",
            "8ead8a4c38e14ed0a3152d0bf7c4e93b",
            "776fd75db4eb45c58e3857242ac6c544",
            "36d5921aa26a495db284c39b87b8952a",
            "42fdfa099bf04088bd8505e59f82c769",
            "1ad26ae2adf741a780c3d61dfc654d5f",
            "b08702ae28a64bcf834e353f93e5c15c",
            "4f5e6af19c5c415790a7ddf52ab6cb58",
            "ea94eb4072044077a7978d16940b8f22"
          ]
        },
        "outputId": "5271684e-e270-4f17-9576-ae768105b571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.87G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f14b458144df4983a4a4f19873b00c02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2213bbac271d4cdc81650686ddc61dd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/948 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0933da2aba3e4a31b2c62f078471a2a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c48985f99cb547389f4536e7c4453a90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47cbbc21616147e8bedec1c6cf170201"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91822aea5ff74fc396af181260e19211"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "### Changing the model here is forbidden !\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-2-7b-bnb-4bit\",    ### Do not change the model for any other models or quantization versions\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "Add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72d2604-6172-422e-eea6-916199a03e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.2.15 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "################# TODO : Tweak the LoRA adapter hyperparameters here.  #####################\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.\n",
        "    lora_alpha = 64,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128\n",
        "#適當適當增加參數可以提高學習能力\n",
        "\n",
        "################# TODO  ####################################################################\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",  ### Use llama-3.1 template for better performance here\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preperation (Loading and Refining)"
      ],
      "metadata": {
        "id": "48fl3jE0UC-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Filtering & Sorting"
      ],
      "metadata": {
        "id": "fLAk-ZCOUP1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, load_from_disk\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_from_disk(\"/content/ML_Spring2025_HW5/fastchat_alpaca_52k\")\n",
        "\n",
        "# ---------------------------\n",
        "# Add a \"text\" field to each example\n",
        "# ---------------------------\n",
        "# This function extracts the first assistant message from the conversation\n",
        "def add_text_field(example):\n",
        "    # Extract the first message where role == 'assistant'\n",
        "    assistant_texts = [msg[\"content\"] for msg in example[\"conversations\"] if msg[\"role\"] == \"assistant\"]\n",
        "    text = assistant_texts[0] if assistant_texts else \"\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Map the function over the dataset to add the \"text\" column.\n",
        "dataset = dataset.map(add_text_field)\n",
        "\n",
        "# Print the dataset structure to confirm the new feature.\n",
        "print(dataset)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "#################### TODO : Define a helper function for computing conversation length ###############\n",
        "\n",
        "# The default \"conversation length\" here refers to the length of the input (human) and output (gpt), you can modify it at your will\n",
        "\n",
        "def compute_conversation_length(example):\n",
        "    # Compute total word count across all messages in the 'conversations' field\n",
        "    return sum(len(message[\"content\"].split()) for message in example[\"conversations\"])\n",
        "\n",
        "\n",
        "#################### TODO ############################################################################\n",
        "\n",
        "# ---------------------------\n",
        "# Simple Sorting Method  (Default)\n",
        "# ---------------------------\n",
        "# Sort the dataset from shortest to longest conversation (by word count)\n",
        "sorted_dataset_simple_list = sorted(dataset, key=compute_conversation_length, reverse=False)\n",
        "\n",
        "# Convert back to a Dataset object\n",
        "sorted_dataset_simple = Dataset.from_list(sorted_dataset_simple_list)\n",
        "\n",
        "print(\"\\nTop examples sorted by simple conversation length:\")\n",
        "for entry in sorted_dataset_simple.select(range(5)):\n",
        "    print(f\"ID: {entry['id']}, Conversation Length: {compute_conversation_length(entry)}\")\n",
        "# ---------------------------\n",
        "\n",
        "\n",
        "\n",
        "############## Advanced Sorting Method (TODO : Modify the sorting key ##################\n",
        "# ---------------------------\n",
        "# Default : Sorting based on Combining conversation length with the 'score' field using a weighted sum.\n",
        "# Here, we multiply the score by 10 and add it to the conversation length.\n",
        "def advanced_sort_key(example):\n",
        "    # 計算對話長度（總詞彙數）\n",
        "    conversation_len = compute_conversation_length(example)\n",
        "\n",
        "    score = example.get(\"score\", 0)\n",
        "    assistant_content = []\n",
        "    human_content = []\n",
        "\n",
        "    for msg in example[\"conversations\"]:\n",
        "        if \"role\" in msg:\n",
        "            if msg[\"role\"] == \"assistant\":\n",
        "                assistant_content.append(msg[\"content\"])\n",
        "            elif msg[\"role\"] == \"user\":\n",
        "                human_content.append(msg[\"content\"])\n",
        "        elif \"from\" in msg:\n",
        "            if msg[\"from\"] == \"gpt\" or msg[\"from\"] == \"assistant\":\n",
        "                assistant_content.append(msg[\"value\"])\n",
        "            elif msg[\"from\"] == \"human\" or msg[\"from\"] == \"user\":\n",
        "                human_content.append(msg[\"value\"])\n",
        "\n",
        "    assistant_len = sum(len(content.split()) for content in assistant_content) if assistant_content else 0\n",
        "    human_len = sum(len(content.split()) for content in human_content) if human_content else 0\n",
        "\n",
        "    # 中等長度回應且分數高的樣本\n",
        "    length_factor = 0\n",
        "    if 50 <= assistant_len <= 200:  # 偏好中等長度的回應\n",
        "        length_factor = 1\n",
        "    elif assistant_len < 30 or assistant_len > 300:  # 懲罰過短或過長的回應\n",
        "        length_factor = -1\n",
        "\n",
        "    return (score * 5) + length_factor + (0.01 * human_len)\n",
        "\n",
        "####################################### TODO ###########################################\n",
        "\n",
        "sorted_dataset_advanced_list = sorted(dataset, key=advanced_sort_key, reverse=True)\n",
        "# Convert back to a Dataset object\n",
        "sorted_dataset_advanced = Dataset.from_list(sorted_dataset_advanced_list)\n",
        "\n",
        "print(\"\\nTop examples sorted by advanced key (combination of conversation length and score):\")\n",
        "for entry in sorted_dataset_advanced.select(range(5)):\n",
        "    print(f\"ID: {entry['id']}, Advanced Key Value: {advanced_sort_key(entry)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "c3b4c5a41d01443e91208258f384d567",
            "437bcdde40ab4602bf7914ac74cd9953",
            "931b7b4dfc1d4cc79593fcaf8d6a3876",
            "c307d166f36a46ed847d8e87617086b1",
            "e2b5ae39c31e48ec8f940cf0c48cad63",
            "7521052267c042e2a1e684299fcee587",
            "b0000a8711aa450790f871d386634c08",
            "c5f6922ab36d4c38bd59ba30a54e8c4c",
            "4d0bd6eeafc94ac886bb68be3bde1487",
            "e7449aef444c406287ab708d6dcad739",
            "f64f5ae93dff44708c14e0f7b57c1764"
          ]
        },
        "id": "yzY4Mvp8UAMq",
        "outputId": "21c8a4c1-bd43-4bd7-c4c9-d2cfa6d59902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/52002 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3b4c5a41d01443e91208258f384d567"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'conversations', 'score', 'text'],\n",
            "    num_rows: 52002\n",
            "})\n",
            "\n",
            "Top examples sorted by simple conversation length:\n",
            "ID: identity_23488, Conversation Length: 22\n",
            "ID: identity_717, Conversation Length: 23\n",
            "ID: identity_1347, Conversation Length: 23\n",
            "ID: identity_1790, Conversation Length: 23\n",
            "ID: identity_2502, Conversation Length: 23\n",
            "\n",
            "Top examples sorted by advanced key (combination of conversation length and score):\n",
            "ID: identity_28441, Advanced Key Value: 26.47\n",
            "ID: identity_41354, Advanced Key Value: 26.42\n",
            "ID: identity_35374, Advanced Key Value: 25.85\n",
            "ID: identity_12177, Advanced Key Value: 25.52\n",
            "ID: identity_9297, Advanced Key Value: 24.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note : You are limited to use 100 sorted data among the 1000 data in the given dataset, no more than 100 data is allowed for training !!!"
      ],
      "metadata": {
        "id": "0BVedOtLHIHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_used = \"sorted_dataset_advanced\"\n",
        "\n",
        "# 選出最高品質的 100 筆數據\n",
        "if dataset_used == \"sorted_dataset_simple\":\n",
        "    # 使用簡單排序，但選擇較理想長度的樣本\n",
        "    middle_start = len(sorted_dataset_simple) // 4  # 避免太短的樣本\n",
        "    train_dataset = sorted_dataset_simple.select(range(middle_start, middle_start + 100))\n",
        "else:\n",
        "    # 使用進階排序並選擇前 100 名\n",
        "    train_dataset = sorted_dataset_advanced.select(range(0, 100))\n",
        "\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "train_dataset = standardize_sharegpt(train_dataset)\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "9Nb7pR889vy8",
        "outputId": "4ce0c215-2f1f-419a-c775-e33747cdb969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "142fbda7e811454396d8a0458b81c9bf",
            "6aeedf15d9414455976caf5cbeeccc6b",
            "a0c34882fef447969ffc2c737aa8c582",
            "1c2fed13c0524e4e9f6c60549b1cd259",
            "2306aa76c25b4f57932f8f0159b09795",
            "5f379e8a22cc4edd8861e3f892df0968",
            "73a833ccf5db4b37954e98e492c97d12",
            "e9e5860b00bd46f384553bdeb8c6799c",
            "fe351f1bd723454a9c3ebad4b6d0bcb4",
            "df11dfa48ac444d585495bfb105ae3ea",
            "fa0c675cbb0947c094ac74bba60d941d",
            "fc74cc1722fb475091858a6ca43e64d9",
            "ce29fe4374c5414fb1340e4206b24943",
            "09092b1c7253478e9dcce7009da564bf",
            "6abd9a57734e4e58b42f190dc3ad7b40",
            "f9f182cca2034d25ad3147aebb559b2b",
            "0cb2cad08fc34dd5b21fa0161f3d9d5e",
            "c9cc85814a964758bed3be8335b542a2",
            "c6e464ffd3114e079d44c8472b95ca02",
            "9217f926bcf24360b3b52bd5c04a919b",
            "6e4fbb38bb1e4bc787d9adbddbdc39d5",
            "6fe34d291d79465ba115dbe50d3173f2"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Standardizing format:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "142fbda7e811454396d8a0458b81c9bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc74cc1722fb475091858a6ca43e64d9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Visualize"
      ],
      "metadata": {
        "id": "l0n1YmtVUYxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGFzmplrEy9I",
        "outputId": "5f831d50-c996-4d03-900a-5db3b4d1e6ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Identify the odd one out. ### Input: Twitter, Instagram, Telegram',\n",
              "  'role': 'user'},\n",
              " {'content': 'Telegram', 'role': 'assistant'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfzTdMtvGE6w"
      },
      "source": [
        "And we see how the chat template transformed these conversations.\n",
        "\n",
        "**[Notice]** Llama 3.1 Instruct's default chat template default adds `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, so do not be alarmed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhXv0xFMGNKE",
        "outputId": "f9a95d75-ac96-45cf-f3a9-baec766b0e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Telegram'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "5TPVyGbPUdJM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL",
        "outputId": "6906797b-2e77-48d9-8209-8023b03d484a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "b46dd5ce83f143b3a0f7da02df1cf473",
            "d8f2a8f9b1434941ad9c803aed66a324",
            "2d0f68d5196c4d2292aab1d1fc4d7019",
            "301bd70e01ad479c9ee117ecf6896111",
            "209f8524de8c468587a6d5d70f250902",
            "7f5ed49852cd452dbffdf8599c1213e9",
            "430d0dd9dbbf479f993b21e21b853ee8",
            "562ac8158cc84a9a85e67dbda0c50800",
            "cffbbf77459f472784eb75fa8b1f0c2a",
            "03673b7edec444369575c731973467c6",
            "bc583db4da8f4e748f34a9779ae9b098",
            "d2d0bf7639b041459489d7230b9600a0",
            "3a1fb38eaf794572a38c5b2bf6c9c3a3",
            "9907d9067b8e49b794937fe058f11b72",
            "4defa236662c4ed5b98cb2b67b0e254f",
            "4027230bd3a3460a8e30fc19f5b3bfcb",
            "92eaadc8bb3a4494a25e84588bc9e50b",
            "dd46e662789f4973b63e491b165d4b42",
            "4a209ba8dbe241a69adcc9f8e8e74c04",
            "08c64367820a4b4b9bcd995d5e0427de",
            "b3e43f6035b24a19838adc7846b2f895",
            "aa117e44912144d5b41e00a815a1a365",
            "ad1c8fabfaa249b2b5d4c6300d07a985",
            "9e6c55fe0f484545bcf4b6fc7f6a5e13",
            "b46762e2c90b4d6498f62479e49386df",
            "89906010c030401880a769a03ad50838",
            "cd86dda84fd24183b61a2358a72fdef1",
            "4cff838614144b648f7796b72b452978",
            "9f1bb5b6c4ea44bbbd5058bbcc8115b1",
            "6de507e33b8940e6a43e6e50665c1fe1",
            "e33c0f821d344ec79c123649f0468fb9",
            "648e17c80e2840fd98979656e0de2d55",
            "23d3093aa23143909299c72783629c60",
            "f26a400370ed47a2b4b4b93fb71a479d",
            "49302d3cec1d4e3aa579b9a60cb430da",
            "a899faf67dea4358a1d2d7f60d2d9b22",
            "9f240fdef6444baf9a60db1c417dd206",
            "54b9e5b7a5b54df9ad09ec00c47998f5",
            "b80ea59f294941d1b8a0d4b4492ee449",
            "232c2d2c823147c18ac939b09d3a321c",
            "a6b1c9c6ffe94252bfa55a2c795eca67",
            "6f3ddbb9af684d629dfe88ae78f6eb9b",
            "5da8b12e173640a087448f290c85ed09",
            "397947f02f6844dc9e7c0e3a95e165ab"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting train dataset to ChatML (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b46dd5ce83f143b3a0f7da02df1cf473"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying chat template to train dataset (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2d0bf7639b041459489d7230b9600a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad1c8fabfaa249b2b5d4c6300d07a985"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f26a400370ed47a2b4b4b93fb71a479d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "\n",
        "################# TODO : Tweak the training hyperparameters here.  #####################\n",
        "\n",
        "\n",
        "training_config = {\n",
        "    \"per_device_train_batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"warmup_steps\": 5,\n",
        "    \"num_train_epochs\": 15,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"optim\": \"adamw_8bit\",\n",
        "    \"weight_decay\": 0.03,\n",
        "    \"lr_scheduler_type\": \"linear\",\n",
        "    \"seed\": 3407,   ### Do not modify the seed for reproducibility\n",
        "}\n",
        "\n",
        "\n",
        "################# TODO #################################################################\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = training_config[\"per_device_train_batch_size\"],\n",
        "        gradient_accumulation_steps = training_config[\"gradient_accumulation_steps\"],\n",
        "        warmup_steps = training_config[\"warmup_steps\"],\n",
        "        num_train_epochs = training_config[\"num_train_epochs\"], # Set this for 1 full training run.\n",
        "        # max_steps = 60,\n",
        "        learning_rate = training_config[\"learning_rate\"],\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = training_config[\"optim\"],\n",
        "        weight_decay = training_config[\"weight_decay\"],\n",
        "        lr_scheduler_type = training_config[\"lr_scheduler_type\"],\n",
        "        seed = training_config[\"seed\"],\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juQiExuBG5Bt",
        "outputId": "2ed90d93-a45e-418a-9191-59fa7c56bec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ff39fa581d324be484d3e6ca94d90372",
            "6057440fdf244847b0b828e4a7597cae",
            "6e6d336bf11a493db4e265cc2c658825",
            "89db0e5b1c4349e8a1490cb057639c86",
            "31eef747c1b5425cb0fa04fc82bf3702",
            "fac4d1e13bd44832ad6f10910ddf5051",
            "be02d480b61c48ffa0137e2895b111cf",
            "aea45220b17e45f78fbc1289a0de7a2a",
            "8e40356713ce4b198199e932278b3bb0",
            "ce234ee97d8c488ab93a65704b6f6bd3",
            "999ba4657db447b0bbbc3d834bbec56a"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff39fa581d324be484d3e6ca94d90372"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "outputId": "77f93ceb-c8fe-4c5b-d1bb-3d7e99bbb281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 100 | Num Epochs = 15\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 8\n",
            "\\        /    Total batch size = 8 | Total steps = 180\n",
            " \"-____-\"     Number of trainable parameters = 79,953,920\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [180/180 32:43, Epoch 13/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.170500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.288900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.955900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.761000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.205400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.948400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.850600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.058000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.089400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.057300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.866900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.979700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.751100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.655100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.875400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.618300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.716000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.383300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.698500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.547100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.753700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.750600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.850900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.817200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.537200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.439000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.368100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.367400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.517200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.324400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.396900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.295300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.290500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.260200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.280300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.184200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.107900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.253400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.118900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.122800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.148200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.101800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.105100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.098100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.115300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.138400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.161500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.112600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.050600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.052600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.028000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.045200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.052800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.028700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.031000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.069200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.102500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.073400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.078000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.036400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.043800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.028400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.016300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.018300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.053400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.030300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.025600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.009800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.035900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.034200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.007500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.017900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.060400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.026500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.006200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.022000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.029200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.048800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.017000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.009800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.030400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.027600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.005900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.009100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.004100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.033000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.027600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.006600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.004000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.016600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.004100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.003500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.019400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.003900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.003900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.004300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TODO : Curriculum Training  (Optional)\n",
        "start training the LLM with “easier” examples (e.g., shorter, clearer conversations) and progressively introduce more complex ones.\n",
        "\n",
        "The total data amount used to train should still not exceed 100 data."
      ],
      "metadata": {
        "id": "rgII2E4bO97F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############## TODO : Curriculum Training  ######################\n",
        "\n",
        "### E.g.\n",
        "### Step 1. Train on sorted_dataset_simple\n",
        "### Step 2. Train on sorted_dataset_advanced"
      ],
      "metadata": {
        "id": "SBG_FsG7PNvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "## Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_true_output(text):\n",
        "    \"\"\"\n",
        "    Extracts the true assistant output from the decoded model output.\n",
        "\n",
        "    It looks for the assistant header token:\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    and extracts everything after it until the first occurrence of \"<|eot_id|>\".\n",
        "    If the assistant header is not found, it falls back to the last occurrence\n",
        "    of \"<|end_header_id|>\\n\\n\". If \"<|eot_id|>\" is not found, the extraction\n",
        "    continues until the end of the string.\n",
        "    \"\"\"\n",
        "    assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    start_index = text.find(assistant_header)\n",
        "    if start_index != -1:\n",
        "        start_index += len(assistant_header)\n",
        "    else:\n",
        "        # Fallback: use the last occurrence of the generic header ending\n",
        "        generic_header = \"<|end_header_id|>\\n\\n\"\n",
        "        start_index = text.rfind(generic_header)\n",
        "        if start_index != -1:\n",
        "            start_index += len(generic_header)\n",
        "        else:\n",
        "            start_index = 0\n",
        "\n",
        "    end_index = text.find(\"<|eot_id|>\", start_index)\n",
        "    if end_index == -1:\n",
        "        end_index = len(text)\n",
        "    return text[start_index:end_index].strip()"
      ],
      "metadata": {
        "id": "JR7YHn1AZLbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# Load the test set JSON file (without GPT responses)\n",
        "with open(\"/content/ML_Spring2025_HW5/test_set_evol_instruct_150.json\", \"r\") as infile:\n",
        "    test_data = json.load(infile)\n",
        "\n",
        "# Dictionary to store inference results\n",
        "inference_results = {}\n",
        "\n",
        "# Loop over each data entry in the test set\n",
        "for index,entry in enumerate(test_data):\n",
        "    entry_id = entry.get(\"id\", \"unknown_id\")\n",
        "\n",
        "    # Build the messages list from the human conversation entries\n",
        "    # (Test set is expected to have only \"human\" messages)\n",
        "    messages = []\n",
        "    for conv in entry.get(\"conversations\", []):\n",
        "        if conv.get(\"from\") == \"human\":\n",
        "            messages.append({\"role\": \"user\", \"content\": conv.get(\"value\", \"\")})\n",
        "        else:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": conv.get(\"value\", \"\")})\n",
        "\n",
        "    # Create inputs using the chat template (required for generation)\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,  # Must add for generation\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "\n",
        "################# TODO : Tweak Decoding Parameters here.  #####################\n",
        "\n",
        "\n",
        "    # Generate model outputs\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        do_sample=True,\n",
        "        max_new_tokens=256,\n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        top_p = 0.92,\n",
        "        top_k = 40,\n",
        "    )\n",
        "\n",
        "\n",
        "################# TODO  ##########################################################\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
        "\n",
        "    # Parse each output to extract the true assistant response\n",
        "    parsed_outputs = [parse_true_output(output) for output in decoded_outputs]\n",
        "\n",
        "    # Store the result for the current entry\n",
        "    inference_results[entry_id] = {\n",
        "        \"input\": messages,\n",
        "        \"output\": parsed_outputs\n",
        "    }\n",
        "\n",
        "    print(f\"Inference completed for entry {entry_id}\")\n",
        "\n",
        "\n",
        "#Write the inference results to the prediction JSON file\n",
        "with open(f\"pred.json\", \"w\") as outfile:\n",
        "    json.dump(inference_results, outfile, indent=4)\n",
        "with open(f\"training_config.json\", \"w\") as outfile:\n",
        "    json.dump(training_config, outfile, indent=4)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/pred.json')\n",
        "\n",
        "print(\"Inference completed for all entries in the test set.\")"
      ],
      "metadata": {
        "id": "loCPSANHR_wZ",
        "outputId": "11f4d260-1bcf-4f2e-cf68-08bcc0c3618e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference completed for entry identity_8174\n",
            "Inference completed for entry identity_16675\n",
            "Inference completed for entry identity_51749\n",
            "Inference completed for entry identity_53196\n",
            "Inference completed for entry identity_65799\n",
            "Inference completed for entry identity_31686\n",
            "Inference completed for entry identity_25291\n",
            "Inference completed for entry identity_31699\n",
            "Inference completed for entry identity_30359\n",
            "Inference completed for entry identity_67085\n",
            "Inference completed for entry identity_60450\n",
            "Inference completed for entry identity_3070\n",
            "Inference completed for entry identity_36778\n",
            "Inference completed for entry identity_50478\n",
            "Inference completed for entry identity_20143\n",
            "Inference completed for entry identity_8300\n",
            "Inference completed for entry identity_45513\n",
            "Inference completed for entry identity_62606\n",
            "Inference completed for entry identity_38166\n",
            "Inference completed for entry identity_22233\n",
            "Inference completed for entry identity_54369\n",
            "Inference completed for entry identity_39141\n",
            "Inference completed for entry identity_15124\n",
            "Inference completed for entry identity_19488\n",
            "Inference completed for entry identity_66774\n",
            "Inference completed for entry identity_1646\n",
            "Inference completed for entry identity_938\n",
            "Inference completed for entry identity_13380\n",
            "Inference completed for entry identity_20131\n",
            "Inference completed for entry identity_59938\n",
            "Inference completed for entry identity_32956\n",
            "Inference completed for entry identity_32515\n",
            "Inference completed for entry identity_40418\n",
            "Inference completed for entry identity_53845\n",
            "Inference completed for entry identity_5949\n",
            "Inference completed for entry identity_8678\n",
            "Inference completed for entry identity_36333\n",
            "Inference completed for entry identity_16311\n",
            "Inference completed for entry identity_55093\n",
            "Inference completed for entry identity_49371\n",
            "Inference completed for entry identity_62747\n",
            "Inference completed for entry identity_1758\n",
            "Inference completed for entry identity_4555\n",
            "Inference completed for entry identity_53149\n",
            "Inference completed for entry identity_56125\n",
            "Inference completed for entry identity_47214\n",
            "Inference completed for entry identity_17337\n",
            "Inference completed for entry identity_21990\n",
            "Inference completed for entry identity_59710\n",
            "Inference completed for entry identity_58792\n",
            "Inference completed for entry identity_43526\n",
            "Inference completed for entry identity_54221\n",
            "Inference completed for entry identity_41687\n",
            "Inference completed for entry identity_49588\n",
            "Inference completed for entry identity_60653\n",
            "Inference completed for entry identity_25370\n",
            "Inference completed for entry identity_10581\n",
            "Inference completed for entry identity_24752\n",
            "Inference completed for entry identity_26314\n",
            "Inference completed for entry identity_69928\n",
            "Inference completed for entry identity_48622\n",
            "Inference completed for entry identity_69088\n",
            "Inference completed for entry identity_34275\n",
            "Inference completed for entry identity_47707\n",
            "Inference completed for entry identity_16427\n",
            "Inference completed for entry identity_3696\n",
            "Inference completed for entry identity_44369\n",
            "Inference completed for entry identity_67251\n",
            "Inference completed for entry identity_43324\n",
            "Inference completed for entry identity_14506\n",
            "Inference completed for entry identity_55612\n",
            "Inference completed for entry identity_38504\n",
            "Inference completed for entry identity_212\n",
            "Inference completed for entry identity_41110\n",
            "Inference completed for entry identity_11932\n",
            "Inference completed for entry identity_39966\n",
            "Inference completed for entry identity_60276\n",
            "Inference completed for entry identity_68912\n",
            "Inference completed for entry identity_52637\n",
            "Inference completed for entry identity_11474\n",
            "Inference completed for entry identity_37568\n",
            "Inference completed for entry identity_4687\n",
            "Inference completed for entry identity_44577\n",
            "Inference completed for entry identity_55142\n",
            "Inference completed for entry identity_42855\n",
            "Inference completed for entry identity_44485\n",
            "Inference completed for entry identity_69256\n",
            "Inference completed for entry identity_68094\n",
            "Inference completed for entry identity_11188\n",
            "Inference completed for entry identity_43756\n",
            "Inference completed for entry identity_30073\n",
            "Inference completed for entry identity_18350\n",
            "Inference completed for entry identity_23194\n",
            "Inference completed for entry identity_12048\n",
            "Inference completed for entry identity_3455\n",
            "Inference completed for entry identity_36517\n",
            "Inference completed for entry identity_44691\n",
            "Inference completed for entry identity_9677\n",
            "Inference completed for entry identity_2281\n",
            "Inference completed for entry identity_11769\n",
            "Inference completed for entry identity_41764\n",
            "Inference completed for entry identity_13733\n",
            "Inference completed for entry identity_55557\n",
            "Inference completed for entry identity_63707\n",
            "Inference completed for entry identity_22872\n",
            "Inference completed for entry identity_32258\n",
            "Inference completed for entry identity_18632\n",
            "Inference completed for entry identity_29514\n",
            "Inference completed for entry identity_6860\n",
            "Inference completed for entry identity_3319\n",
            "Inference completed for entry identity_2140\n",
            "Inference completed for entry identity_39471\n",
            "Inference completed for entry identity_10759\n",
            "Inference completed for entry identity_5721\n",
            "Inference completed for entry identity_50510\n",
            "Inference completed for entry identity_28099\n",
            "Inference completed for entry identity_18060\n",
            "Inference completed for entry identity_12298\n",
            "Inference completed for entry identity_49746\n",
            "Inference completed for entry identity_63808\n",
            "Inference completed for entry identity_40739\n",
            "Inference completed for entry identity_3363\n",
            "Inference completed for entry identity_65554\n",
            "Inference completed for entry identity_54315\n",
            "Inference completed for entry identity_393\n",
            "Inference completed for entry identity_69938\n",
            "Inference completed for entry identity_42000\n",
            "Inference completed for entry identity_27410\n",
            "Inference completed for entry identity_29274\n",
            "Inference completed for entry identity_704\n",
            "Inference completed for entry identity_12567\n",
            "Inference completed for entry identity_23393\n",
            "Inference completed for entry identity_33134\n",
            "Inference completed for entry identity_37372\n",
            "Inference completed for entry identity_32509\n",
            "Inference completed for entry identity_8768\n",
            "Inference completed for entry identity_68493\n",
            "Inference completed for entry identity_8640\n",
            "Inference completed for entry identity_23939\n",
            "Inference completed for entry identity_21111\n",
            "Inference completed for entry identity_50901\n",
            "Inference completed for entry identity_23500\n",
            "Inference completed for entry identity_36745\n",
            "Inference completed for entry identity_26063\n",
            "Inference completed for entry identity_35418\n",
            "Inference completed for entry identity_68496\n",
            "Inference completed for entry identity_57544\n",
            "Inference completed for entry identity_25443\n",
            "Inference completed for entry identity_47264\n",
            "Inference completed for entry identity_631\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_07981ae6-dcb4-40db-8356-d6aa09d44b2d\", \"pred.json\", 216764)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference completed for all entries in the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving, loading finetuned models"
      ],
      "metadata": {
        "id": "rSz-CYXjIXVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the model"
      ],
      "metadata": {
        "id": "OLDgDmo5IqBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "1k568kj8IJXe",
        "outputId": "43504c96-c011-4406-cc9d-726260cd348b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.model',\n",
              " 'lora_model/added_tokens.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model"
      ],
      "metadata": {
        "id": "pPYojUG-Ivtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_model\", # The folder path containing of the folder that contains adapter_model.safetensors, adapter_config.json and README.md\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "nFi97oG5Iz2v",
        "outputId": "eed1d9e8-d860-4393-e67a-73636f2686d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=11008, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}