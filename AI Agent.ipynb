{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0hrDTjq13nQhzqBAWPxak",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyo0914/ML2025/blob/main/2025hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWooAAULNoyO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ML/HW1"
      ],
      "metadata": {
        "id": "DZbxgZyKNs6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1oNRKKr6Ns3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ],
      "metadata": {
        "id": "ZZ_eXzGPGvXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=10) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]"
      ],
      "metadata": {
        "id": "VOFW1baoGvz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ],
      "metadata": {
        "id": "yD41SOxoGvxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 問題提取代理：從複雜描述中提取核心問題，保留所有關鍵資訊\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一位精煉提問的專家。你只負責提取問題的核心，去除冗餘描述。如果原問題已經簡潔，則直接返回原問題。絕對不要嘗試回答問題或猜測答案。也不要自己隨意生成原問題沒有的文字。\",\n",
        "    task_description=\"###任務###\\n請從以下輸入中提取出核心問題，去除冗餘描述，但保留所有關鍵專有名詞和時間地點人物等重要資訊，注意引號的內容要保留。如果原問題已經簡潔明確，則直接原封不動地返回原問題。注意：你的任務僅是提煉問題，絕對不是回答問題或猜測答案。\\n\\n###輸入###\\n\"\n",
        ")\n",
        "\n",
        "# 關鍵詞提取代理：提取最佳搜索關鍵詞，強調專有名詞\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是頂尖的搜索關鍵詞提取專家。你精通從問題中提取最適合用於Google搜尋的關鍵詞和短語，特別擅長保留所有專有名詞、地名、人名等核心實體。並且不可以生成出原本問題裡面沒有的關鍵字。\",\n",
        "    task_description=\"###任務###\\n請從以下問題中提取3-5個最適合用於Google搜尋的關鍵詞或短語。這些關鍵詞應該能幫助找到最相關的資訊。只返回關鍵詞，以空格分隔，不要有任何其他文字或標點符號。請特別注意保留所有專有名詞（如人名、地名、機構名、產品名等），這些是搜尋的核心。\\n\\n###問題###\\n\",\n",
        ")\n",
        "\n",
        "# 問答代理：根據提供的資訊回答問題，強調事實準確性\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是一位專業的回答專家。你會根據得問題和得到的資訊來判斷出最有可能的答案，提供最精簡的回答，但是同時也要精準(例如行政區劃的範圍不能過大)。你會直接給出答案，不需要解釋資訊來源。對於數字、日期、人名、地名等事實性資訊，你會特別確保其準確性。使用中文時只會使用繁體中文來回答問題。\",\n",
        "    task_description=\"###任務###\\n請根據提供的資訊來回答以下問題。你的回答必須明確且精準。不用解釋你的思考過程或資訊來源。優先使用搜尋結果中的資訊回答，特別關注專有名詞、數字、日期等具體事實資訊。確保數字和專有名詞的準確性是最高優先級。若是地名則給出詳細的鄉鎮市。如果搜尋結果中找不到答案，則基於你的知識提供最可能的答案。如果是未來事件或無法確定的資訊，請明確表示無法提供答案。\\n\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "7gdqLF13Gvvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def improved_pipeline(question: str) -> str:\n",
        "    try:\n",
        "        # Step 1: 提取核心問題\n",
        "        print(f\"處理問題: {question}\")\n",
        "        simplified_question = question_extraction_agent.inference(question)\n",
        "        print(f\"1.關鍵問題: {simplified_question}\")\n",
        "\n",
        "        # Step 2: 使用簡化問題進行搜尋(限3個結果)\n",
        "        simplified_search_results = []\n",
        "        try:\n",
        "            print(f\"使用簡化問題進行搜尋: {simplified_question}\")\n",
        "            simplified_search_results = await search(simplified_question, n_results=3)\n",
        "            print(f\"使用簡化問題搜尋到 {len(simplified_search_results)} 個結果\")\n",
        "\n",
        "            # 打印簡化問題搜尋結果預覽\n",
        "            for i, result in enumerate(simplified_search_results, 1):\n",
        "                print(f\"簡化問題搜尋結果 {i}:\")\n",
        "                print(f\"內容預覽: {result[:300]}...\")\n",
        "        except Exception as e:\n",
        "            print(f\"簡化問題搜尋錯誤: {e}\")\n",
        "\n",
        "        # 過濾有效的搜尋結果\n",
        "        valid_simplified_results = [r for r in simplified_search_results if len(r) > 100]\n",
        "\n",
        "        # 確保結果不會太長，以防模型上下文長度限制\n",
        "        search_results = [result[:8000] for result in valid_simplified_results]\n",
        "\n",
        "        final_answer = None\n",
        "\n",
        "        # 只有在簡化問題搜尋無結果時才提取關鍵詞\n",
        "        keywords = None\n",
        "\n",
        "        if search_results:\n",
        "            # 有簡化問題搜尋結果，準備上下文並生成答案\n",
        "            context = \"以下是從網路搜尋到的資訊：\\n\\n\"\n",
        "            for i, result in enumerate(search_results, 1):\n",
        "                context += f\"搜尋結果 {i}：\\n{result}\\n\\n\"\n",
        "\n",
        "            # 限制上下文長度\n",
        "            max_context_len = 15000\n",
        "            if len(context) > max_context_len:\n",
        "                context = context[:max_context_len] + \"...(資訊被截斷)\"\n",
        "\n",
        "            print(f\"3.參考的內容: \\n{context}\")\n",
        "\n",
        "            # 關鍵修改：使用原始問題而非簡化問題來生成答案\n",
        "            qa_prompt = f\"問題：{question}\\n\\n{context}\"\n",
        "            final_answer = qa_agent.inference(qa_prompt)\n",
        "\n",
        "        else:\n",
        "            # 簡化問題無搜尋結果，使用關鍵字搜尋\n",
        "            print(\"簡化問題無搜尋結果，嘗試使用關鍵字搜尋...\")\n",
        "\n",
        "            # 提取搜尋關鍵詞\n",
        "            keywords = keyword_extraction_agent.inference(simplified_question)\n",
        "            print(f\"2.關鍵詞: {keywords}\")\n",
        "\n",
        "            keyword_results = []\n",
        "            try:\n",
        "                print(f\"使用關鍵詞進行搜尋: {keywords}\")\n",
        "                keyword_results = await search(keywords, n_results=3)\n",
        "                print(f\"使用關鍵字搜尋到 {len(keyword_results)} 個結果\")\n",
        "\n",
        "                # 打印關鍵詞搜尋結果\n",
        "                for i, result in enumerate(keyword_results, 1):\n",
        "                    print(f\"關鍵詞搜尋結果 {i}:\")\n",
        "                    print(f\"內容預覽: {result[:300]}...\")\n",
        "            except Exception as e:\n",
        "                print(f\"關鍵字搜尋錯誤: {e}\")\n",
        "\n",
        "            valid_keyword_results = [r for r in keyword_results if len(r) > 100]\n",
        "            keyword_search_results = [result[:8000] for result in valid_keyword_results]\n",
        "\n",
        "            if keyword_search_results:\n",
        "                # 準備關鍵字搜尋結果上下文\n",
        "                keyword_context = \"以下是從網路搜尋到的資訊：\\n\\n\"\n",
        "                for i, result in enumerate(keyword_search_results, 1):\n",
        "                    keyword_context += f\"搜尋結果 {i}：\\n{result}\\n\\n\"\n",
        "\n",
        "                if len(keyword_context) > max_context_len:\n",
        "                    keyword_context = keyword_context[:max_context_len] + \"...(資訊被截斷)\"\n",
        "\n",
        "                print(f\"3.參考的內容: \\n{keyword_context}\")\n",
        "\n",
        "                # 關鍵修改：使用原始問題而非簡化問題來生成答案\n",
        "                keyword_qa_prompt = f\"問題：{question}\\n\\n{keyword_context}\"\n",
        "                final_answer = qa_agent.inference(keyword_qa_prompt)\n",
        "            else:\n",
        "                # 所有搜尋都無結果，使用模型知識生成答案\n",
        "                print(\"所有搜尋都無結果，使用模型知識生成答案...\")\n",
        "                # 關鍵修改：使用原始問題而非簡化問題\n",
        "                final_answer = qa_agent.inference(f\"問題：{question}\")\n",
        "                print(\"3.參考的內容: 無搜尋結果，使用模型知識\")\n",
        "\n",
        "        # 確保答案不為空\n",
        "        if not final_answer or len(final_answer.strip()) == 0:\n",
        "            print(\"答案為空，使用模型知識生成答案...\")\n",
        "            # 關鍵修改：使用原始問題而非簡化問題\n",
        "            final_answer = qa_agent.inference(f\"問題：{question}\")\n",
        "            print(\"3.參考的內容: 無有效答案，使用模型知識\")\n",
        "\n",
        "        print(f\"4.最終答案: {final_answer}\")\n",
        "        return final_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Pipeline錯誤: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        # 返回直接答案作為備用方案\n",
        "        return qa_agent.inference(question)"
      ],
      "metadata": {
        "id": "cprKYbxQGvtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "STUDENT_ID = \"r13044045\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        # 移除檔案存在的檢查條件，直接處理所有問題並覆寫\n",
        "        print(f\"\\n處理問題 {id}: {question}\")\n",
        "        answer = await improved_pipeline(question)  # 使用improved_pipeline\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(f\"問題 {id} 的答案: {answer}\")\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:  # 使用'w'模式覆寫\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        # 移除檔案存在的檢查條件，直接處理所有問題並覆寫\n",
        "        print(f\"\\n處理問題 {id}: {question}\")\n",
        "        answer = await improved_pipeline(question)  # 使用improved_pipeline\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(f\"問題 {id} 的答案: {answer}\")\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:  # 使用'w'模式覆寫\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "DvyzmtC6GvrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:  # 'w'模式會覆寫現有文件\n",
        "    for id in range(1, 91):\n",
        "        try:\n",
        "            with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "                answer = input_f.readline().strip()\n",
        "                print(answer, file=output_f)\n",
        "        except FileNotFoundError:\n",
        "            # 如果某個答案文件不存在，可以添加一個空行或默認答案\n",
        "            print(f\"無法找到問題{id}的答案\", file=output_f)\n",
        "            # 或者直接跳過: continue"
      ],
      "metadata": {
        "id": "ImkY7seDGvo4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}